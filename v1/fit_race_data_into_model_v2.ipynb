{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pymysql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import category_encoders\n",
    "import joblib\n",
    "\n",
    "from Config import params_config, query_config, db_config\n",
    "from Utils.bulk_insert import BulkInsert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_race_info_into_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = query_config.queries\n",
    "parameters = params_config.parameters\n",
    "db_params = db_config.db_params\n",
    "con = pymysql.connect(**db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchall_and_make_list_by(query, con):\n",
    "    try:\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(query)\n",
    "        fetch_result = cursor.fetchall()\n",
    "        fetch_result_list = [item for item in fetch_result]\n",
    "        cursor.close()\n",
    "        return fetch_result_list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def get_training_race_data_frame(queries, parameters, con):\n",
    "    selected_query = queries['TRAINING_DATA_FROM_MASTER_PRIOR_RESULT']\n",
    "    training_race_data_list = fetchall_and_make_list_by(selected_query, con)\n",
    "    training_race_data_frame = pd.DataFrame(training_race_data_list, \n",
    "                                          columns=parameters['DATAFRAME_COL_NAMES']['training_race_data_cols'])\n",
    "    return training_race_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df =  get_training_race_data_frame(queries, parameters, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_race_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_year_month_day_from_race_timing(x):\n",
    "    date_str = re.match('([0-9]+)/([0-9]+)/([0-9]+)' , x).group()\n",
    "    year = datetime.datetime.strptime(date_str, '%Y/%m/%d').year\n",
    "    month = datetime.datetime.strptime(date_str, '%Y/%m/%d').month\n",
    "    day = datetime.datetime.strptime(date_str, '%Y/%m/%d').day\n",
    "    return pd.Series([year, month, day])\n",
    "\n",
    "def _get_dow_from_race_timing(x):\n",
    "    return re.search(\"土|日\" , x).group() \n",
    "\n",
    "def _encode_dow(df):\n",
    "    dow_mapping = {'土': 1, '日': 2}\n",
    "    return df['dow'].map(dow_mapping)    \n",
    "\n",
    "def _get_time_in_the_racecourse_from_race_timing(x):\n",
    "    return int(re.split('([0-9]+)回([ぁ-んァ-ン 一-龥]+)([0-9]+)日目' , x)[1])\n",
    "\n",
    "def _get_racecourse_from_race_timing(x):\n",
    "    return re.split('([0-9]+)回([ぁ-んァ-ン 一-龥]+)([0-9]+)日目' , x)[2]\n",
    "\n",
    "def _get_what_day_in_the_racecourse_from_race_timing(x):\n",
    "    return int(re.split('([0-9]+)回([ぁ-んァ-ン 一-龥]+)([0-9]+)日目' , x)[3])\n",
    "\n",
    "def _encode_race_course(df):\n",
    "    race_course_mapping = {'函館': 1, '札幌': 2, '福島': 3, '東京': 4, '中山': 5, '新潟': 6, '中京': 7, '阪神': 8, '京都': 9, '小倉': 10}\n",
    "    return df['race_course'].map(race_course_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_race_timing(df):\n",
    "    df[['year', 'month', 'day']] = df['race_timing'].apply(_get_year_month_day_from_race_timing)\n",
    "    df['dow'] = df['race_timing'].apply(_get_dow_from_race_timing)\n",
    "    df['dow_encoded'] = _encode_dow(df)\n",
    "    df['race_course'] =  df['race_timing'].apply(_get_racecourse_from_race_timing)\n",
    "    df['race_course_encoded'] = _encode_race_course(df)\n",
    "    df['time_in_racecourse'] =  df['race_timing'].apply(_get_time_in_the_racecourse_from_race_timing)\n",
    "    df['what_day_in_racecourse'] =  df['race_timing'].apply(_get_what_day_in_the_racecourse_from_race_timing)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df = preprocess_race_timing(training_race_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_race_weather(df):\n",
    "    race_weather_mapping = {'晴': 1, '曇': 2, '小雨': 3, '雨': 4, '小雪': 5, '雪':6, 'unknown':7}\n",
    "    return df['race_weather'].map(race_weather_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_race_df['race_weather_encoded'] = encode_race_weather(training_race_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_race_condition(df):\n",
    "    race_condition_mapping = {'良': 1, '稍': 2, '重': 3, '不': 4, 'unknown':5}\n",
    "    return df['race_condition'].map(race_condition_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_race_df['race_condition_encoded'] = encode_race_condition(training_race_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fit_and_transform_href_to_the_horse(df):\n",
    "    if parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_FOR_HORSE']=='TargetEncoder':\n",
    "        ce = category_encoders.TargetEncoder(cols=['href_to_the_horse'])\n",
    "    elif parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_FOR_HORSE']=='OrdinalEncoder':\n",
    "        ce = category_encoders.OrdinalEncoder(cols=['href_to_the_horse'])\n",
    "        \n",
    "    ce.fit(df, \n",
    "           df[parameters['DATAFRAME_COL_NAMES']['target_col']],\n",
    "           handle_unknown=parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_HANDLE_UNKNOWN'])\n",
    "    joblib.dump(ce, parameters['FILE_NAME_OF_HORSE_CATEGORY_ENCODERS'])\n",
    "    \n",
    "    df_ce = ce.transform(df)\n",
    "    df_ce = df_ce.rename(columns={'href_to_the_horse': 'href_to_the_horse_encoded'})\n",
    "    return pd.concat([df, df_ce['href_to_the_horse_encoded']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ce_loaded = joblib.load(parameters['FILE_NAME_OF_CATEGORY_ENCODERS'])\n",
    "# ce_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df = encode_fit_and_transform_href_to_the_horse(training_race_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from prior or result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_horse_age_and_sex_in_result(x):\n",
    "    horse_sex = re.split('([ぁ-んァ-ン 一-龥]+)([0-9]+)' , x)[1]\n",
    "    horse_age = int(re.split('([ぁ-んァ-ン 一-龥]+)([0-9]+)' , x)[2])\n",
    "    return pd.Series([horse_sex, horse_age])\n",
    "\n",
    "def  _encode_horse_sex(df_about_horse_sex):\n",
    "    horse_sex_mapping = {'牡': 1, '牝': 2, 'セ': 3}\n",
    "    return df_about_horse_sex.map(horse_sex_mapping)\n",
    "\n",
    "def preprocess_horse_sex_age(df, target_cols_type):\n",
    "    if target_cols_type == 'result':\n",
    "        df[['horse_sex', 'horse_age']] = df['horse_sex_age_in_result'].apply(_get_horse_age_and_sex_in_result)\n",
    "        df['horse_sex_encoded'] = _encode_horse_sex(df['horse_sex'])\n",
    "    elif target_cols_type == 'prior':\n",
    "        df['horse_age'] = pd.to_numeric(training_race_df[\"horse_age_in_prior\"], errors='coerce')\n",
    "        df['horse_sex_encoded'] = _encode_horse_sex(df['horse_sex_in_prior'])      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df = preprocess_horse_sex_age(df=training_race_df, target_cols_type='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_race_df = training_race_df[training_race_df['horse_weight_in_result']!='計不(---)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_horse_weight_increment(x):\n",
    "    return int(x.replace('＋', '+').replace('－', '-').replace('---', '0'))\n",
    "\n",
    "def _get_horse_weight_info_in_result(x):\n",
    "    horse_weight = int(re.split('(\\()(.*)(\\))' , x)[0])\n",
    "    horse_weight_increment_str = re.split('(\\()(.*)(\\))' , x)[2]\n",
    "    horse_weight_increment = _parse_horse_weight_increment(horse_weight_increment_str)\n",
    "    return pd.Series([horse_weight, horse_weight_increment])\n",
    "\n",
    "def _get_horse_weight_in_prior(x):\n",
    "    try:\n",
    "        return int(re.search(\"[0-9]+\" , x).group())\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "def _get_horse_weight_increment_in_prior(x):\n",
    "    try:\n",
    "        horse_weight_increment_str = re.split('(\\()(.*)(kg\\))' , x)[2]\n",
    "        horse_weight_increment = _parse_horse_weight_increment(horse_weight_increment_str)\n",
    "        return horse_weight_increment\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "def preprocess_horse_weight_and_increment(df, target_cols_type):\n",
    "    if target_cols_type == 'result':\n",
    "        df[['horse_weight', 'horse_weight_increment']] = df['horse_weight_in_result'].apply(_get_horse_weight_info_in_result)\n",
    "    elif target_cols_type == 'prior':\n",
    "        df['horse_weight'] = df['horse_weight_in_prior'].apply(_get_horse_weight_in_prior)\n",
    "        df['horse_weight_increment'] = df['horse_weight_increment_in_prior'].apply(_get_horse_weight_increment_in_prior)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df = preprocess_horse_weight_and_increment(df=training_race_df, target_cols_type='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_and_encode_weight_loss_flg(x):\n",
    "    try:\n",
    "        weight_loss_flg = re.search('▲|△|☆' , x).group()\n",
    "        weight_loss_encode = int(weight_loss_flg.replace('▲', '3').replace('△', '2').replace('☆', '1'))\n",
    "    except AttributeError:\n",
    "        weight_loss_encode = 0\n",
    "    return weight_loss_encode\n",
    "\n",
    "def _get_horse_impost_in_prior(x):\n",
    "    try:\n",
    "        return float(re.split('(▲|△|☆|.)(.*)(\\()(.*)(\\))(.*)' , x)[4])\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "def _get_weight_loss_encode_in_prior(x):\n",
    "    try:\n",
    "        weight_loss_flg_str = re.split('(▲|△|☆|.)(.*)(\\()(.*)(\\))(.*)' , x)[1]\n",
    "        return _get_and_encode_weight_loss_flg(weight_loss_flg_str)\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "def preprocess_jockey_name(df, target_cols_type):\n",
    "    if target_cols_type == 'result':\n",
    "        df['horse_impost'] = df['horse_impost_in_result']\n",
    "        df['weight_loss_encode'] = df['jockey_name_in_result'].apply(_get_and_encode_weight_loss_flg)\n",
    "    elif target_cols_type == 'prior':\n",
    "        df['horse_impost'] = df['jockey_name_and_horse_impost_in_prior'].apply(_get_horse_impost_in_prior)\n",
    "        df['weight_loss_encode'] = df['jockey_name_and_horse_impost_in_prior'].apply(_get_weight_loss_encode_in_prior)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df = preprocess_jockey_name(df=training_race_df, target_cols_type='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fit_and_transform_href_to_the_jockey(df):\n",
    "    if parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_FOR_JOCKEY']=='TargetEncoder':\n",
    "        ce = category_encoders.TargetEncoder(cols=['href_to_the_jockey'])\n",
    "    elif parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_FOR_JOCKEY']=='OrdinalEncoder':\n",
    "        ce = category_encoders.OrdinalEncoder(cols=['href_to_the_jockey'])\n",
    "        \n",
    "    ce.fit(df, \n",
    "           df[parameters['DATAFRAME_COL_NAMES']['target_col']],\n",
    "           handle_unknown=parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_HANDLE_UNKNOWN'])\n",
    "    joblib.dump(ce, parameters['FILE_NAME_OF_JOCKEY_CATEGORY_ENCODERS'])\n",
    "    \n",
    "    df_ce = ce.transform(df)\n",
    "    df_ce = df_ce.rename(columns={'href_to_the_jockey': 'href_to_the_jockey_encoded'})\n",
    "    return pd.concat([df, df_ce['href_to_the_jockey_encoded']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ce_loaded = joblib.load(parameters['FILE_NAME_OF_JOCKEY_CATEGORY_ENCODERS'])\n",
    "# ce_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_race_df = encode_fit_and_transform_href_to_the_jockey(training_race_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_trainer_belonging_in_result(x):\n",
    "    return re.split('\\[(.*)\\]' , x)[1]\n",
    "\n",
    "def _get_trainer_belonging_in_prior(x):\n",
    "    try:\n",
    "        return re.split('(.*)(・)(.*)' , x)[1]\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "def _encode_trainer_belonging(df):\n",
    "    trainer_belonging_mapping = {'美': 1, '栗': 2, '招': 3}\n",
    "    return df['trainer_belonging'].map(trainer_belonging_mapping)\n",
    "\n",
    "def preprocess_trainer_name(df, target_cols_type):\n",
    "    if target_cols_type == 'result':\n",
    "        df['trainer_belonging'] = df['trainer_name_in_result'].apply(_get_trainer_belonging_in_result)\n",
    "        df['trainer_belonging_encoded'] = _encode_trainer_belonging(df)\n",
    "    elif target_cols_type == 'prior':\n",
    "        df['trainer_belonging'] = df['trainer_name_in_prior'].apply(_get_trainer_belonging_in_prior)\n",
    "        df['trainer_belonging_encoded'] = _encode_trainer_belonging(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_race_df = preprocess_trainer_name(df=training_race_df, target_cols_type='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_fit_and_transform_href_to_the_trainer(df):\n",
    "#     if parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_FOR_TRAINER']=='TargetEncoder':\n",
    "#         ce = category_encoders.TargetEncoder(cols=['href_to_the_trainer'])\n",
    "#     elif parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_FOR_TRAINER']=='OrdinalEncoder':\n",
    "#         ce = category_encoders.OrdinalEncoder(cols=['href_to_the_trainer'])\n",
    "        \n",
    "#     ce.fit(df, \n",
    "#            df[parameters['DATAFRAME_COL_NAMES']['target_col']],\n",
    "#            handle_unknown=parameters['HYPER_PARAMETERS']['CATEGORY_ENCODERS_HANDLE_UNKNOWN'])\n",
    "#     joblib.dump(ce, parameters['FILE_NAME_OF_TRAINER_CATEGORY_ENCODERS'])\n",
    "    \n",
    "#     df_ce = ce.transform(df)\n",
    "#     df_ce = df_ce.rename(columns={'href_to_the_trainer': 'href_to_the_trainer_encoded'})\n",
    "#     return pd.concat([df, df_ce['href_to_the_trainer_encoded']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_race_df = encode_fit_and_transform_href_to_the_trainer(training_race_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _categorize_arrival_order(x):\n",
    "    if x == 1:\n",
    "        arrival_order_category = parameters['MODEL_TARGET_RANK_LABEL']['first']\n",
    "    elif x == 2:\n",
    "        arrival_order_category = parameters['MODEL_TARGET_RANK_LABEL']['second']\n",
    "    elif x == 3:\n",
    "        arrival_order_category = parameters['MODEL_TARGET_RANK_LABEL']['third']\n",
    "    else:\n",
    "        arrival_order_category = parameters['MODEL_TARGET_RANK_LABEL']['others']\n",
    "    return arrival_order_category\n",
    "\n",
    "def preprocess_arrival_order(df):\n",
    "    df['arrival_order_category'] = df['arrival_order'].apply(_categorize_arrival_order)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_race_df = preprocess_arrival_order(df=training_race_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check wether Preprocess Class works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.Preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessing(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_result_data_based_training_race_df(df, pp):\n",
    "    df = pp.preprocess_race_timing(df=df)\n",
    "    df = pp.encode_race_weather(df=df)\n",
    "    df = pp.encode_race_condition(df=df)\n",
    "    df = pp.encode_fit_and_transform_href_to_the_horse(df=df)\n",
    "    df = pp.preprocess_horse_sex_age(df=df, target_cols_type='result')\n",
    "    df = pp.preprocess_horse_weight_and_increment(df=df, target_cols_type='result')\n",
    "    df = pp.preprocess_jockey_name(df=df, target_cols_type='result')\n",
    "    df = pp.encode_fit_and_transform_href_to_the_jockey(df=df)\n",
    "    df = pp.preprocess_trainer_name(df=df, target_cols_type='result')\n",
    "    df = pp.preprocess_arrival_order(df=df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df_preprocessed = preprocess_result_data_based_training_race_df(training_race_df, pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_race_df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_to_model_fit(df):\n",
    "    train_df = df[(df['year']<parameters['CRITERIA_TO_SPLIT_TRAINING_DATA']['year']) | (df['month']<parameters['CRITERIA_TO_SPLIT_TRAINING_DATA']['month'])]\n",
    "    validataion_df = df[(df['year']>=parameters['CRITERIA_TO_SPLIT_TRAINING_DATA']['year']) & (df['month']>=parameters['CRITERIA_TO_SPLIT_TRAINING_DATA']['month'])]\n",
    "    \n",
    "    x_train = np.array(train_df[parameters['DATAFRAME_COL_NAMES']['feature_cols_part1']])\n",
    "    group_train = np.array(train_df[parameters['DATAFRAME_COL_NAMES']['query_cols']])\n",
    "    y_train = np.array(train_df[parameters['DATAFRAME_COL_NAMES']['target_col']])\n",
    "    x_valid = np.array(validataion_df[parameters['DATAFRAME_COL_NAMES']['feature_cols_part1']])\n",
    "    group_valid = np.array(validataion_df[parameters['DATAFRAME_COL_NAMES']['query_cols']])\n",
    "    y_valid = np.array(validataion_df[parameters['DATAFRAME_COL_NAMES']['target_col']])\n",
    "    \n",
    "    return x_train, group_train,  y_train, x_valid, group_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, group_train,  y_train, x_valid, group_valid, y_valid = make_dataset_to_model_fit(df=training_race_df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(group_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(group_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).groupby(y_train_df.values).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_valid).groupby(y_valid_df.values).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case when no tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "rf_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_valid_pred = rf_clf.predict(x_valid)\n",
    "pd.Series(y_valid_pred).groupby(pd.Series(y_valid_pred).values).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"HYPER_PARAMETERS\"]['RF_CLF'] = {\n",
    "    'CV_WAYS': 'GridSearchCV',  # 'GridSearchCV', 'RandomizedSearchCV'\n",
    "    'GS_PARAMS': {'n_estimators': [10, 50, 100], \n",
    "                                 'max_depth': [5, 10, 20], \n",
    "                                 'max_features': ['sqrt', 'log2', None],\n",
    "                                 'class_weight': ['balanced', None]},\n",
    "    'RS_PARAMS': {'n_estimators': sp_randint(100, 5000), \n",
    "                                 'max_depth': sp_randint(5, 50), \n",
    "                                 'max_features': ['sqrt', 'log2', None],\n",
    "                                 'class_weight': ['balanced', None]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case when RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=0),\n",
    "                                    param_distributions=parameters[\"HYPER_PARAMETERS\"]['RF_CLF']['RS_PARAMS'],\n",
    "                                    n_iter=30, #54,\n",
    "                                    scoring=\"roc_auc\",\n",
    "                                    cv=3,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,          \n",
    "                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                               n_estimators=cv.best_params_['n_estimators'],\n",
    "                               max_depth=cv.best_params_['max_depth'],\n",
    "                               max_features=cv.best_params_['max_features'],\n",
    "                               class_weight=cv.best_params_['class_weight'])\n",
    "rf_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = rf_clf.predict(x_valid)\n",
    "print(classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'class_weight': 'balanced',\n",
    " 'max_depth': 26,\n",
    " 'max_features': 'log2',\n",
    " 'n_estimators': 982}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = rf_clf.predict(x_valid)\n",
    "print(classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case when GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(estimator=RandomForestClassifier(random_state=0),\n",
    "                    param_grid=parameters[\"HYPER_PARAMETERS\"]['RF_CLF']['GS_PARAMS'],\n",
    "                    scoring=\"f1_micro\",\n",
    "                    cv=3,\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=0,\n",
    "                               n_estimators=cv.best_params_['n_estimators'],\n",
    "                               max_depth=cv.best_params_['max_depth'],\n",
    "                               max_features=cv.best_params_['max_features'],\n",
    "                               class_weight=cv.best_params_['class_weight'])\n",
    "rf_clf.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = rf_clf.predict(x_valid)\n",
    "print(classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make evaluation func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_valid_pred_proba = rf_clf.predict_proba(x_valid)\n",
    "y_valid_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_valid_df, y_valid_pred_proba[:, 1])\n",
    "auc_score = auc(fpr, tpr)\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc_score)\n",
    "plt.legend()\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score_by_first_order(y_flag: np.array, y_pred_proba: np.array, group_ids: np.array):\n",
    "    y_combined = np.c_[y_flag, y_pred_proba]\n",
    "    y_flag_proba = np.empty((0, 2))\n",
    "    for race_id in pd.unique(group_ids):\n",
    "        y_combined_by_race = y_combined[group_ids==race_id]\n",
    "        y_combined_by_race_sorted = y_combined_by_race[y_combined_by_race[:,-1].argsort()[::-1]]\n",
    "        y_combined_by_race_sorted = np.c_[y_combined_by_race_sorted, np.zeros(len(y_combined_by_race_sorted))]\n",
    "        y_combined_by_race_sorted[0, -1] = 1\n",
    "        y_flag_proba = np.append(y_flag_proba, y_combined_by_race_sorted[:, (0,-1)], axis=0)\n",
    "    \n",
    "    return y_flag_proba, f1_score(y_flag_proba[:, 0], y_flag_proba[:, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_flag_proba, f1_pred_score = calc_score_by_first_order(y_flag=y_valid, y_pred_proba=y_valid_pred_proba, group_ids=group_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_flag_proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame(np.c_[np.array(x_train_df.columns), rf_clf.feature_importances_],\n",
    "                                    columns=['features', 'importance']).sort_values(by=['importance'], ascending=False)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Learning to Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(D, 10)\n",
    "        self.l2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listnet_loss(y_i, z_i):\n",
    "    \"\"\"\n",
    "    y_i: (n_i, 1)\n",
    "    z_i: (n_i, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    P_y_i = F.softmax(y_i, dim=0)\n",
    "    P_z_i = F.softmax(z_i, dim=0)\n",
    "    return - torch.sum(P_y_i * torch.log(P_z_i))\n",
    "\n",
    "def make_dataset(N_train, N_valid, D):\n",
    "    ws = torch.randn(D, 1)\n",
    "\n",
    "    X_train = torch.randn(N_train, D, requires_grad=True)\n",
    "    X_valid = torch.randn(N_valid, D, requires_grad=True)\n",
    "\n",
    "    ys_train_score = torch.mm(X_train, ws)\n",
    "    ys_valid_score = torch.mm(X_valid, ws)\n",
    "\n",
    "    bins = [-2, -1, 0, 1]  # 5 relevances\n",
    "    ys_train_rel = torch.Tensor(\n",
    "        np.digitize(ys_train_score.clone().detach().numpy(), bins=bins)\n",
    "    )\n",
    "    ys_valid_rel = torch.Tensor(\n",
    "        np.digitize(ys_valid_score.clone().detach().numpy(), bins=bins)\n",
    "    )\n",
    "\n",
    "    return X_train, X_valid, ys_train_rel, ys_valid_rel\n",
    "\n",
    "\n",
    "def swapped_pairs(ys_pred, ys_target):\n",
    "    N = ys_target.shape[0]\n",
    "    swapped = 0\n",
    "    for i in range(N - 1):\n",
    "        for j in range(i + 1, N):\n",
    "            if ys_target[i] < ys_target[j]:\n",
    "                if ys_pred[i] > ys_pred[j]:\n",
    "                    swapped += 1\n",
    "            elif ys_target[i] > ys_target[j]:\n",
    "                if ys_pred[i] < ys_pred[j]:\n",
    "                    swapped += 1\n",
    "    return swapped\n",
    "\n",
    "\n",
    "def ndcg(ys_true, ys_pred):\n",
    "    def dcg(ys_true, ys_pred):\n",
    "        _, argsort = torch.sort(ys_pred, descending=True, dim=0)\n",
    "        ys_true_sorted = ys_true[argsort]\n",
    "        ret = 0\n",
    "        for i, l in enumerate(ys_true_sorted, 1):\n",
    "            ret += (2 ** l - 1) / np.log2(1 + i)\n",
    "        return ret\n",
    "    ideal_dcg = dcg(ys_true, ys_true)\n",
    "    pred_dcg = dcg(ys_true, ys_pred)\n",
    "    return pred_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 500\n",
    "N_valid = 100\n",
    "D = 50\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "X_train, X_valid, ys_train, ys_valid = make_dataset(N_train, N_valid, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(D)\n",
    "opt = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(N_train)\n",
    "\n",
    "X_train = X_train[idx]\n",
    "ys_train = ys_train[idx]\n",
    "\n",
    "cur_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X = X_train[cur_batch: cur_batch + batch_size]\n",
    "batch_ys = ys_train[cur_batch: cur_batch + batch_size]\n",
    "cur_batch += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred = net(batch_X)\n",
    "batch_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = listnet_loss(batch_ys, batch_pred)\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
